---
title: "supportR Vignette"
author: "Nicholas J Lyon"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{supportR Vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r knitr-mechanics, include = F}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

```{r pre-setup, echo = FALSE, message = FALSE}
# devtools::install_github("njlyon0/supportR")
```

## Overview

The `supportR` package is an amalgam of distinct functions I've written to accomplish small data wrangling, quality control, or visualization tasks. These functions tend to be short and narrowly-defined. An additional consequence of the motivation for creating them is that they tend to not be inter-related or united by a common theme. If this vignette feels somewhat scattered because of that, I hope it doesn't negatively affect how informative it is or your willingness to adopt `supportR` into your scripts!

This vignette describes the main functions of `supportR` using the examples included in each function.

```{r setup}
#install.packages("supportR")
library(supportR)
```

### Data Wrangling

In order to demonstrate some of the data wrangling functions of `supportR`, we'll use some some example data from Dr. [Allison Horst](https://allisonhorst.com/)'s [`palmerpenguins` R package](https://github.com/allisonhorst/palmerpenguins).

```{r ex-data-pngn}
# Load library
library(palmerpenguins)

# Glimpse the penguins dataset
str(penguins)
```

With that data loaded, we can use the `summary_table` function to quickly get group-wise summaries and retrieve generally useful summary statistics.

The `groups` argument supports a vector of all of the column names to group by while `response` must be a single numeric column. The `drop_na` argument allows group combinations that result in an NA to be automatically dropped (i.e., if a penguin didn't have an island listed that would be dropped). The mean, standard deviation (SD), sample size, and standard error (SE) are all returned to facilitate easy figure creation. There is also a `round_digits` argument that lets you specify how many digits you'd like to retain for the mean, SD, and SE.

```{r summary_table}
# Summarize the data
supportR::summary_table(data = penguins, groups = c("species", "island"),
                        response = "bill_length_mm", drop_na = T)
```

`crop_tri` allows dropping one "triangle" of a symmetric dataframe / matrix. It also includes a `drop_diag` argument that accepts a logical for whether to drop the diagonal of the data object. This is primarily useful (I find) in allowing piping through this function as opposed to using the base R notation for removing a triangle of a symmetric data object.


```{r crop_tri}
# Define a simple matrix wtih symmetric dimensions
mat <- matrix(data = c(1:2, 2:1), nrow = 2, ncol = 2)

# Crop off it's lower triangle
supportR::crop_tri(data = mat, drop_tri = "lower", drop_diag = FALSE)

# Drop the diagonal as well
supportR::crop_tri(data = mat, drop_tri = "lower", drop_diag = TRUE)
```

`array_melt` allows users to 'melt' an array of dimensions X, Y, and Z into a dataframe containing columns "x", "y", "z", and "value" where "value" is whatever was stored at those coordinates in the array.

```{r array_melt}
# Make data to fill the array
vec1 <- c(5, 9, 3)
vec2 <- c(10:15)

# Create dimension names (x = col, y = row, z = which matrix)
x_vals <- c("Col_1","Col_2","Col_3")
y_vals <- c("Row_1","Row_2","Row_3")
z_vals <- c("Mat_1","Mat_2")

# Make an array from these components
g <- array(data = c(vec1, vec2), dim = c(3, 3, 2),
           dimnames = list(x_vals, y_vals, z_vals))

# "Melt" the array into a dataframe
melted <- supportR::array_melt(array = g)

# Look at that top of that
head(melted)
```

### Quality Control

In terms of quality control functions, `diff_check` compares two vectors and reports back what is in the first but not the second (i.e., what is "lost") and what is in the second but not the first (i.e., what is "gained"). I find this most useful (A) when comparing the index columns of two data objects I intend to join together and (B) to ensure no columns are unintentionally removed during lengthy `tidyverse`-style  pipes (`%>%`).

`diff_check` also includes optional logical arguments `sort` and `return` that will respectively either sort the difference in both vectors and return a two-element if set to `TRUE`.

```{r diff_check}
# Make two vectors
vec1 <- c("x", "a", "b")
vec2 <- c("y", "z", "a")

# Compare them!
supportR::diff_check(old = vec1, new = vec2, sort = TRUE, return = TRUE)
```

This package also includes the function `num_check` that identifies all values of a column that would be coerced to `NA` if `as.numeric` was run on the column. Once these non-numbers are identified you can handle that in whatever way you feel is most appropriate. `num_check` is intended only to flag these for your attention, *not* to attempt a fix using a method you may or may not support.

```{r num_check}
# Make a dataframe with non-numbers in a number column
fish <- data.frame('species' = c('salmon', 'bass', 'halibut', 'eel'),
                   'count' = c(1, '14x', '_23', 12))

# Use `num_check` to identify non-numbers
num_check(data = fish, col = "count")
```

`date_check` does a similar operation but is checking a column for entries that would be coerced to `NA` by `as.Date` instead. Note that if a date is sufficiently badly formatted `as.Date` will throw an error instead of coercing to `NA` so `date_check` will do the same thing.

```{r date_check}
# Make a dataframe including malformed dates
sites <- data.frame('site' = c("LTR", "GIL", "PYN", "RIN"),
                    'visit' = c('2021-01-01', '2021-01-0w', '1990', '2020-10-xx'))

# Now we can use our function to identify bad dates
supportR::date_check(data = sites, col = 'visit')
```

Both `num_check` and `date_check` can accept multiple column names to the `col` argument (as of version 1.1.1) and all columns are checked separately.

Another date column quality control function is `date_format_guess`. This This function checks a column of dates (stored as characters!) and tries to guess the format of the date (i.e., month/day/year, day/month/year, etc.).

It can make a more informed guess if there is a grouping column because it can use the frequency of the "date" entries within those groups to guess whether a given number is the day or the month. This is based on the assumption that sampling occurs more often within months than across them so the number that occurs in more rows within the grouping values is most likely month.

Recognizing that assumption may be uncomfortable for some users, the `groups` argument can be set to `FALSE` and it will do the clearer judgment calls (i.e., if a number is >12 it is day, etc.). Note that dates that cannot be guessed by my function will return "FORMAT UNCERTAIN" so that you can handle them using your knowledge of the system (or by returning to your raw data if need be).

```{r date_format_guess}
# Make a dataframe with dates in various formats and a grouping column
my_df <- data.frame('data_enterer' = c('person A', 'person B',
                                       'person B', 'person B',
                                       'person C', 'person D',
                                       'person E', 'person F',
                                       'person G'),
                    'bad_dates' = c('2022.13.08', '2021/2/02',
                                    '2021/2/03', '2021/2/04',
                                    '1899/1/15', '10-31-1901',
                                    '26/11/1901', '08.11.2004',
                                    '6/10/02'))

# Now we can invoke the function!
supportR::date_format_guess(data = my_df, date_col = "bad_dates",
                            group_col = "data_enterer", return = "dataframe")

# If preferred, do it without groups and return a vector
supportR::date_format_guess(data = my_df, date_col = "bad_dates",
                            groups = FALSE, return = "vector")
```

### Data Visualization

I've created a set of custom `ggplot2` `theme` elements to guarantee that all of my figures share similar aesthetics. Feel free to use `theme_lyon` if you have similar preferences!

`theme_lyon` does the following changes to a `ggplot2` plot:

- Removes legend title and background
- Removes gray box behind colors in legend elements
- Removes major/minor gridlines
- Makes axes' lines black
- Increases the font size of the axes titles and tick labels

```{r theme_lyon, message = F, warning = F, fig.width = 5}
# Load ggplot2
library(ggplot2)

# Create a plot and allow default ggplot themeing to be added
ggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +
  geom_boxplot(outlier.shape = 24)

# Compare with the same plot with my theme
ggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +
  geom_boxplot(outlier.shape = 24) +
  supportR::theme_lyon()
```

I've also created `nms_ord` and `pcoa_ord` for Non-Metric Multi-Dimensional Scaling (NMS) & Principal Coordinates Analysis (PCoA) Ordinations respectively.

```{r nms_ord, message = F, warning = F, results = 'hide', fig.width = 5, fig.height = 5}
# Load data from the `vegan` package
utils::data("varespec", package = 'vegan')
resp <- varespec

# Make a columns to split the data into 4 groups
factor_4lvl <- c(rep.int("Trt_1", (nrow(resp)/4)),
                 rep.int("Trt_2", (nrow(resp)/4)),
                 rep.int("Trt_3", (nrow(resp)/4)),
                 rep.int("Trt_4", (nrow(resp)/4)))

# And combine them into a single data object
data <- cbind(factor_4lvl, resp)

# Actually perform multidimensional scaling
mds <- vegan::metaMDS(data[-1], autotransform = FALSE, 
                      expand = FALSE, k = 2, try = 10)

# With the scaled object and original dataframe we can use this function
supportR::nms_ord(mod = mds, groupcol = data$factor_4lvl,
                  title = '4-Level NMS', leg_pos = 'topright',
                  leg_cont = c('1', '2', '3', '4'))
```

`pcoa_ord` has the same syntax as `nms_ord` but it expects an object created by `ape::pcoa` rather than `vegan::metaMDS`.

### Operations Outside of R

Finally, I've written several functions that allow you to interact with APIs outside of R via R functions with hopefully more comfortable syntax. Because these functions rely on user credentials, they cannot be run non-interactively (as in a CRAN submission) so the following code chunks are not evaluated and are included as examples of the proper syntax for your reference.

For GitHub users, I've developed two related functions: `github_ls` and `github_tree`. `github_ls` accepts the URL to a GitHub repository to which you have access (public or private). It creates a dataframe of that repository's contents including their names, types, and full paths within the repository. Listing of a particular folder and recursive listing of all nested subfolders within a repository are both supported via additional arguments.

If the `folder` argument is set to `NULL` (the default) the top level of the repository is listed.

```{r github_ls, eval = F}
# List all files in a GitHub repository
supportR::github_ls(repo = "https://github.com/njlyon0/supportR",
                    recursive = TRUE, quiet = FALSE)

# Or list files in only a particular folder
supportR::github_ls(repo = "https://github.com/njlyon0/supportR", folder = "R",
                    recursive = FALSE, quiet = TRUE)
```

`github_tree` is an extension of `github_ls` that identifies all files in a repository and creates a file tree diagram of that folder structure that is simple and human-readable. Unlike `github_ls`, `github_tree` only supports recursive identification of all files beginning at the top level of the repository. It does however allow users to exclude the listings of particular folders by specifying their names in the `exclude` argument.

I think this could be particularly useful to embed in a repository's `README.Rmd` to create a quick-and-easy file map for visitors to use as a guide in navigating the repository's contents.

```{r github_tree, eval = F}
# Create a file tree diagram of a GitHub repository
supportR::github_tree(repo = repo = "https://github.com/njlyon0/supportR",
                      exclude = c("docs", "man", ".github"), quiet = FALSE)
```

For users who create RMarkdown reports and want to store them in a Google Drive folder,  `rmd_export` knits and exports a given R Markdown file both locally and to a user-designated Google Drive folder. Note that you **_MUST_** authenticate your R session with the `googledrive` package so that it has permission to access the Drive folder you supply. I recommend running `googledrive::drive_auth()` and doing the authentication "dance" in a browser before using `rmd_export` to reduce the chances of any errors.

```{r rmd_export, eval = F}
# Authorize R to interact with GoogleDrive
googledrive::drive_auth()

# Use `rmd_export()` to knit and export an .Rmd file
supportR::rmd_export(rmd = "my_markdown.Rmd",
                     in_path = file.path("Folder in my WD with the .Rmd named in `rmd`"),
                     out_path = file.path("Folder in my WD to save the knit file to"),
                     out_name = "desired name for output",
                     out_type = "html",
                     drive_link = "<Full Google Drive link>")
```

